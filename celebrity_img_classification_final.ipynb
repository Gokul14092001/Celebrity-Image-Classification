{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "celebrity_img_classification_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "MC7OjqLNhUdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "import base64\n",
        "import cv2\n",
        "from wavelet import w2d\n",
        "\n",
        "__class_name_to_number = {}\n",
        "__class_number_to_name = {}\n",
        "\n",
        "__model = None\n",
        "\n",
        "def classify_image(image_base64_data, file_path=None):\n",
        "\n",
        "    imgs = get_cropped_image_if_2_eyes(file_path, image_base64_data)\n",
        "\n",
        "    result = []\n",
        "    for img in imgs:\n",
        "        scalled_raw_img = cv2.resize(img, (32, 32))\n",
        "        img_har = w2d(img, 'db1', 5)\n",
        "        scalled_img_har = cv2.resize(img_har, (32, 32))\n",
        "        combined_img = np.vstack((scalled_raw_img.reshape(32 * 32 * 3, 1), scalled_img_har.reshape(32 * 32, 1)))\n",
        "\n",
        "        len_image_array = 32*32*3 + 32*32\n",
        "\n",
        "        final = combined_img.reshape(1,len_image_array).astype(float)\n",
        "        result.append({\n",
        "            'class': class_number_to_name(__model.predict(final)[0]),\n",
        "            'class_probability': np.around(__model.predict_proba(final)*100,2).tolist()[0],\n",
        "            'class_dictionary': __class_name_to_number\n",
        "        })\n",
        "\n",
        "    return result\n",
        "\n",
        "def class_number_to_name(class_num):\n",
        "    return __class_number_to_name[class_num]\n",
        "\n",
        "def load_saved_artifacts():\n",
        "    print(\"loading saved artifacts...start\")\n",
        "    global __class_name_to_number\n",
        "    global __class_number_to_name\n",
        "\n",
        "    with open(\"class_dictionary.json\", \"r\") as f:\n",
        "        __class_name_to_number = json.load(f)\n",
        "        __class_number_to_name = {v:k for k,v in __class_name_to_number.items()}\n",
        "\n",
        "    global __model\n",
        "    if __model is None:\n",
        "        with open('new_saved_model.pkl', 'rb') as f:\n",
        "            __model = joblib.load(f)\n",
        "    print(\"loading saved artifacts...done\")\n",
        "\n",
        "\n",
        "def get_cv2_image_from_base64_string(b64str):\n",
        "    '''\n",
        "    credit: https://stackoverflow.com/questions/33754935/read-a-base-64-encoded-image-from-memory-using-opencv-python-library\n",
        "    :param uri:\n",
        "    :return:\n",
        "    '''\n",
        "    encoded_data = b64str.split(',')[1]\n",
        "    nparr = np.frombuffer(base64.b64decode(encoded_data), np.uint8)\n",
        "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    return img\n",
        "\n",
        "def get_cropped_image_if_2_eyes(image_path, image_base64_data):\n",
        "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "    eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
        "\n",
        "    if image_path:\n",
        "        img = cv2.imread(image_path)\n",
        "    else:\n",
        "        img = get_cv2_image_from_base64_string(image_base64_data)\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    cropped_faces = []\n",
        "    for (x,y,w,h) in faces:\n",
        "            roi_gray = gray[y:y+h, x:x+w]\n",
        "            roi_color = img[y:y+h, x:x+w]\n",
        "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
        "            if len(eyes) >= 2:\n",
        "                cropped_faces.append(roi_color)\n",
        "    return cropped_faces\n",
        "\n",
        "def get_b64_test_image_for_virat():\n",
        "    with open(\"b64.txt\") as f:\n",
        "        return f.read()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    load_saved_artifacts()\n",
        "\n",
        "    print(classify_image(get_b64_test_image_for_virat(), None))\n",
        "\n",
        "    print(classify_image(None, \"ajith.png\"))#\n",
        "    print(classify_image(None, \"aishwarya1.jpg\"))#\n",
        "    print(classify_image(None, \"cena2.png\")) #\n",
        "    print(classify_image(None, \"msd3.jpg\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuQd_9cSqXPA",
        "outputId": "730264a0-04eb-4aa9-815b-2fbeddb72e2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading saved artifacts...start\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:333: UserWarning: Trying to unpickle estimator Pipeline from version 0.24.1 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading saved artifacts...done\n",
            "[{'class': 'dhoni', 'class_probability': [0.13, 0.29, 99.29, 0.01, 0.28], 'class_dictionary': {'aishwarya rai': 0, 'ajith kumar': 1, 'dhoni': 2, 'john cene': 3, 'smriti mandhana': 4}}]\n",
            "[{'class': 'ajith kumar', 'class_probability': [0.0, 38.71, 9.05, 17.73, 34.51], 'class_dictionary': {'aishwarya rai': 0, 'ajith kumar': 1, 'dhoni': 2, 'john cene': 3, 'smriti mandhana': 4}}]\n",
            "[{'class': 'aishwarya rai', 'class_probability': [47.99, 0.05, 3.11, 47.81, 1.04], 'class_dictionary': {'aishwarya rai': 0, 'ajith kumar': 1, 'dhoni': 2, 'john cene': 3, 'smriti mandhana': 4}}]\n",
            "[{'class': 'john cene', 'class_probability': [0.04, 1.25, 0.05, 69.89, 28.77], 'class_dictionary': {'aishwarya rai': 0, 'ajith kumar': 1, 'dhoni': 2, 'john cene': 3, 'smriti mandhana': 4}}]\n",
            "[{'class': 'dhoni', 'class_probability': [0.09, 0.09, 92.66, 7.11, 0.05], 'class_dictionary': {'aishwarya rai': 0, 'ajith kumar': 1, 'dhoni': 2, 'john cene': 3, 'smriti mandhana': 4}}]\n"
          ]
        }
      ]
    }
  ]
}